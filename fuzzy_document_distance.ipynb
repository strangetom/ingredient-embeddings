{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94e8c26b-312d-45f6-960b-006c22c144db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from functools import lru_cache\n",
    "\n",
    "import floret\n",
    "import nltk.stem.porter as nsp\n",
    "import numpy as np\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ecb70379-65cf-422f-bbaa-9f4643ffdd1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "STEMMER = nsp.PorterStemmer()\n",
    "# Disable stuff we don't need to speed up.\n",
    "NLP = spacy.load(\n",
    "    \"en_core_web_md\",\n",
    "    disable=[\"tok2vec\", \"tagger\", \"parser\", \"attribute_ruler\", \"lemmatizer\", \"ner\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "49c4a036-92d5-44fc-9f02-a2d424dc63b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "@lru_cache(maxsize=512)\n",
    "def stem(token: str, to_lowecase: bool) -> str:\n",
    "    \"\"\"Stem function with cache to improve performance.\n",
    "\n",
    "    The stem of a word output by the PorterStemmer is always the same, so we can\n",
    "    cache the result the first time and return that for subsequent future calls\n",
    "    without the need to do all the processing again.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    token : str\n",
    "        Token to stem\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        Stem of token\n",
    "    \"\"\"\n",
    "    return STEMMER.stem(token, to_lowercase=to_lowecase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "78e3a533-df98-4b49-95d2-e22e9dedddc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare(document: str) -> list[str]:\n",
    "    \"\"\"Prepare recipe for embeddings training.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    ingredients : list[str]\n",
    "        List of recipe ingredients.\n",
    "    instructions : list[str]\n",
    "        List of recipes instructions.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        Prepared recipe.\n",
    "    \"\"\"\n",
    "    doc = NLP(document)\n",
    "\n",
    "    return [\n",
    "        stem(str(token), to_lowecase=True)\n",
    "        for token in doc\n",
    "        if not token.is_punct\n",
    "        and not token.is_currency\n",
    "        and not token.is_digit\n",
    "        and not token.is_space\n",
    "        and not token.is_stop\n",
    "        and not token.like_num\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b5056c4a-e767-4077-8a48-d7ef5a9932d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_similarity(word1: str, word2: str, model) -> float:\n",
    "    \"\"\"Calculate similarity between two word embeddings.\n",
    "\n",
    "    This uses the reciprocal euclidean distance transformed by a\n",
    "    sigmoid function to return a value between 0 and 1.\n",
    "    1 indicates an exact match (i.e. same word).\n",
    "    0 indicates no match whatsoever.\n",
    "\n",
    "    Inputs\n",
    "    ------\n",
    "    word1 : str\n",
    "        First word.\n",
    "    word2 : str\n",
    "        Second word.\n",
    "    model : floret\n",
    "        Embeddings model\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        Value between 0 and 1.\n",
    "    \"\"\"\n",
    "    euclidean_dist = np.linalg.norm(model[word1] - model[word2])\n",
    "\n",
    "    if euclidean_dist == 0:\n",
    "        return 1\n",
    "    elif euclidean_dist == np.inf:\n",
    "        return 0\n",
    "    else:\n",
    "        sigmoid = 1 / (1 + np.exp(-1/euclidean_dist))\n",
    "        return float(sigmoid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "703301c9-eac6-47b6-b272-879988b7aba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def doc_similarity(word: str, document: list[str], model) -> float:\n",
    "    \"\"\"Calculate the similarity of word to document.\n",
    "\n",
    "    Similarlity score is calculated from the euclidean distance between word and\n",
    "    all members of document. The reciprocal of this distance if transformed using\n",
    "    a signmoid function to return the score between 0 and 1.\n",
    "\n",
    "    Inputs\n",
    "    ------\n",
    "    word : str\n",
    "        Word to calculate membership of.\n",
    "    document : str\n",
    "        Document to calculate word membership to.\n",
    "    model : floret\n",
    "        Embeddings model\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        Membership score between 0 and 1, where 1 indicates exact match.\n",
    "    \"\"\"\n",
    "    return max(word_similarity(word, d, model) for d in document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4d94c4c9-b1a4-437f-b064-0b92056a7e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fuzzy_document_distance(document1: list[str], document2: list[str], model):\n",
    "    \"\"\"Calculate fuzzy document distance between two documents.\n",
    "\n",
    "    Implementation of https://doi.org/10.1109/ACCESS.2021.3058559.\n",
    "\n",
    "    Inputs\n",
    "    ------\n",
    "    document1 : list[str]\n",
    "        Tokens for first document.\n",
    "    document2 : list[str]\n",
    "        Tokens for second document.\n",
    "    model : floret\n",
    "        Embeddings model\n",
    "\n",
    "    \"\"\"\n",
    "    # Remove out of vocabularly words\n",
    "    # ! Is this necessary?\n",
    "    document1 = [token for token in document1 if token in model]\n",
    "    document2 = [token for token in document2 if token in model]\n",
    "\n",
    "    # If either document only contains out of vocab words, return infinite distance\n",
    "    if not document1 or not document2:\n",
    "        return float(\"inf\")\n",
    "\n",
    "    # Calculate fuzzy intersection\n",
    "    union_membership = 0.0\n",
    "    cj1_membership = 0.0\n",
    "    cj2_membership = 0.0\n",
    "\n",
    "    tokens = set(document1) | set(document2)\n",
    "    for token in tokens:\n",
    "        union_membership += doc_similarity(token, document1, model) * doc_similarity(\n",
    "            token, document2, model\n",
    "        )\n",
    "        cj1_membership += doc_similarity(token, document1, model)\n",
    "        cj2_membership += doc_similarity(token, document2, model)\n",
    "\n",
    "    res = union_membership / (cj1_membership + cj2_membership - union_membership)\n",
    "    return 1 - res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "39002417-8481-4639-b8ac-6328dea9f7b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = floret.load_model(\"test_embeddings.floret.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "78d86195-f29f-4268-a1c1-a4a6e9aed4cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3849511742591858"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fuzzy_document_distance([\"red\", \"pepper\"], [\"pepper\", \"bell\", \"green\", \"raw\"], model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e3b95777-777b-42ce-909b-effef6d7acab",
   "metadata": {},
   "outputs": [],
   "source": [
    "fdc_ingredients = []\n",
    "with open(\"data/fdc_ingredients.csv\", \"r\") as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    for row in reader:\n",
    "        fdc_ingredients.append(row[\"description\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "143db3ec-9f2f-45c8-afca-786080a0ef28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Onions, red, raw:\t 0.16311589876810706\n",
      "Cabbage, red, raw:\t 0.3611718416213989\n",
      "Onions, yellow, raw:\t 0.3620023876428604\n",
      "Onions, white, raw:\t 0.3627890944480896\n",
      "Peppers, bell, red, raw:\t 0.38058264255523677\n"
     ]
    }
   ],
   "source": [
    "scores = []\n",
    "search = prepare(\"red onion\")\n",
    "for i, ing in enumerate(fdc_ingredients):\n",
    "    prepared_fdc = prepare(ing)\n",
    "    scores.append((fuzzy_document_distance(search, prepared_fdc, model), i))\n",
    "\n",
    "for score, idx in sorted(scores, key=lambda x: x[0])[:5]:\n",
    "    print(f\"{fdc_ingredients[idx]}:\\t {score}\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5241569c-2bdc-4cfc-b2a9-7ab3ce3e17e8",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
